{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting data"
      ],
      "metadata": {
        "id": "ppcFJLA_ZHWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://blog.langchain.dev/semi-structured-multi-modal-rag/\n",
        "https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb?ref=blog.langchain.dev"
      ],
      "metadata": {
        "id": "KZTDiHj-9Dmc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7ZwE32URqxD",
        "outputId": "29ae6757-e66c-44a0-ad2d-9497b418f428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: langchainhub in /usr/local/lib/python3.10/dist-packages (0.1.21)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.34)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.104)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.42.0)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.1)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.32.0.20240712)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.8.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.112.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.6)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.19.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.3)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (30.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.0)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.38.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain_community) (1.33)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.23.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain langchain-chroma \"unstructured[all-docs]\" pydantic lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPKIpQ-DIn1s",
        "outputId": "2af7eaef-2cab-4613-a116-9aad6630cb7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Collecting langchain-chroma\n",
            "  Downloading langchain_chroma-0.1.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: unstructured[all-docs] in /usr/local/lib/python3.10/dist-packages (0.15.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.34)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.104)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma)\n",
            "  Downloading chromadb-0.5.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: fastapi<1,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.112.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.4.27)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.9.6)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.25.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.9.5)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (20231228)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (9.2.0)\n",
            "Requirement already satisfied: python-pptx>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.0.2)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.16.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.3)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.3.13)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.7)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Requirement already satisfied: python-docx>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.1.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.17.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.3.1)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.7.4)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.4.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.5)\n",
            "Requirement already satisfied: pillow-heif in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.1.4)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.13)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.0.1)\n",
            "Requirement already satisfied: unstructured-inference==0.7.36 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.7.36)\n",
            "Requirement already satisfied: layoutparser in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.0.9)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.23.5)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.10.0.84)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (1.19.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (2.3.1+cu121)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.42.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.20.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.30.6)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.5.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.3)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.12.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (30.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.27.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.38.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (10.4.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (0.18.1+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.0.8)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.19.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (2024.5.15)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (43.0.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[all-docs]) (1.2.14)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from python-oxmsg->unstructured[all-docs]) (0.47)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (7.0.1)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.17.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured[all-docs]) (4.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.63.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain) (3.0.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (4.9.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.13.2)\n",
            "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.0.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (71.0.4)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (3.1.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.7.36->unstructured[all-docs]) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.36->unstructured[all-docs]) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.36->unstructured[all-docs]) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.6.20)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.7.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.20.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.23.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (1.13.1)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (0.11.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.22)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.20.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.16.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (10.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (2.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.1.5)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (4.30.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
            "Downloading langchain_chroma-0.1.3-py3-none-any.whl (10 kB)\n",
            "Downloading chromadb-0.5.3-py3-none-any.whl (559 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chroma-hnswlib, chromadb, langchain-chroma\n",
            "  Attempting uninstall: chroma-hnswlib\n",
            "    Found existing installation: chroma-hnswlib 0.7.6\n",
            "    Uninstalling chroma-hnswlib-0.7.6:\n",
            "      Successfully uninstalled chroma-hnswlib-0.7.6\n",
            "  Attempting uninstall: chromadb\n",
            "    Found existing installation: chromadb 0.5.5\n",
            "    Uninstalling chromadb-0.5.5:\n",
            "      Successfully uninstalled chromadb-0.5.5\n",
            "Successfully installed chroma-hnswlib-0.7.3 chromadb-0.5.3 langchain-chroma-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
        "!pip install -U arxiv pymupdf\n",
        "!pip install rapidocr-onnxruntime\n",
        "import fitz\n",
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htFLxPmrUXse",
        "outputId": "a818c15a-c896-4de7-f480-3ef521b96d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.9)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.9 in /usr/local/lib/python3.10/dist-packages (from pymupdf) (1.24.9)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.7.4)\n",
            "Traceback (most recent call last):\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/BRISQUE.pdf'"
      ],
      "metadata": {
        "id": "EI2EzDWeVG6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from typing import Any\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "# Get elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=filepath,\n",
        "    # Using pdf format to find embedded image blocks\n",
        "    extract_images_in_pdf=True,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    # Hard max on chunks\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=\"/content/Untitled Folder\",\n",
        ")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "Gu3wHMoOI2xX",
        "outputId": "17bb312c-750c-402e-cac6-454065765db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from typing import Any\\n\\nfrom pydantic import BaseModel\\nfrom unstructured.partition.pdf import partition_pdf\\n\\n# Get elements\\nraw_pdf_elements = partition_pdf(\\n    filename=filepath,\\n    # Using pdf format to find embedded image blocks\\n    extract_images_in_pdf=True,\\n    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\\n    # Titles are any sub-section of the document\\n    infer_table_structure=True,\\n    # Post processing to aggregate text once we have the title\\n    chunking_strategy=\"by_title\",\\n    # Chunking params to aggregate text blocks\\n    # Attempt to create a new chunk 3800 chars\\n    # Attempt to keep chunks > 2000 chars\\n    # Hard max on chunks\\n    max_characters=4000,\\n    new_after_n_chars=3800,\\n    combine_text_under_n_chars=2000,\\n    image_output_dir_path=\"/content/Untitled Folder\",\\n)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = fitz.open(filepath)\n",
        "\n",
        "#Calculate number of pages in PDF file\n",
        "page_nums = len(pdf_file)\n",
        "\n",
        "#Create empty list to store images information\n",
        "images_list = []\n",
        "\n",
        "#Extract all images information from each page\n",
        "for page_num in range(page_nums):\n",
        "    page_content = pdf_file[page_num]\n",
        "    images_list.extend(page_content.get_images())\n",
        "\n",
        "print(len(images_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-WhiH8uVETH",
        "outputId": "108cb483-1caf-40db-89a6-0b46b3a9e6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(images_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWKGJpQ6og7S",
        "outputId": "1f93d397-c6be-4682-bc45-05e0e55978d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60, 0, 512, 384, 8, 'ICCBased', '', 'Im1', 'DCTDecode')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers_stream_generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw3jiYJBY48s",
        "outputId": "471b03e7-8a51-4963-f060-8bd1bc5c5863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers_stream_generator\n",
            "  Using cached transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.10/dist-packages (from transformers_stream_generator) (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.26.1->transformers_stream_generator) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.26.1->transformers_stream_generator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2024.7.4)\n",
            "Building wheels for collected packages: transformers_stream_generator\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12425 sha256=ca5084e12cf185a2da5e6eef8cde8da53fbdd3bd99dbc10bf0440d30361337f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/4a/90/140f7b67d125906f6a165f38aad212ecb4a695ad0d87582437\n",
            "Successfully built transformers_stream_generator\n",
            "Installing collected packages: transformers_stream_generator\n",
            "Successfully installed transformers_stream_generator-0.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForImageCaptioning\n",
        "from PIL import Image\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForImageCaptioning.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "1Al6r4ndbr5E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "70dba56b-952f-4fd0-8040-ab72a67ac2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AutoModelForImageCaptioning' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-946652c4e65c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForImageCaptioning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForImageCaptioning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoModelForImageCaptioning' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, image in enumerate(images_list, start=1):\n",
        "    #Extract the image object number\n",
        "    xref = image[0]\n",
        "    #Extract image\n",
        "    base_image = pdf_file.extract_image(xref)\n",
        "    #Store image bytes\n",
        "    image_bytes = base_image['image']\n",
        "    #Store image extension\n",
        "    image_ext = base_image['ext']\n",
        "    #Generate image file name\n",
        "    image_name = str(i) + '.' + image_ext\n",
        "    #Save image\n",
        "    with open(os.path.join(\"/content/\", image_name) , 'wb') as image_file:\n",
        "        image_file.write(image_bytes)\n",
        "        image_file.close()"
      ],
      "metadata": {
        "id": "ae_VzY-AVOeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        # Load and preprocess the image\n",
        "        image_path = \"/content/11.png\"\n",
        "        import cv2\n",
        "        from google.colab.patches import cv2_imshow\n",
        "\n",
        "        img = cv2.imread(image_path)\n",
        "        cv2_imshow(img)\n",
        "        cv2.waitKey(0)\n",
        "        inputs = processor(images=img, return_tensors=\"pt\")\n",
        "        # Generate caption\n",
        "        outputs = model.generate(**inputs)\n",
        "        caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        print(\"Generated Summary:\", caption)\n",
        "        cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "Pbtej7MhViy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install python-dotenv\n",
        "%pip install \"unstructured[all-docs]\" unstructured-client watermark\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv(find_dotenv())"
      ],
      "metadata": {
        "id": "X-U7PPwsWBdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from IPython.display import JSON\n",
        "\n",
        "import json\n",
        "\n",
        "from unstructured_client import UnstructuredClient\n",
        "from unstructured_client.models import shared\n",
        "from unstructured_client.models.errors import SDKError\n",
        "\n",
        "from unstructured.partition.html import partition_html\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.staging.base import dict_to_elements, elements_to_json\n",
        "%load_ext watermark\n",
        "\n",
        "\n",
        "import unstructured.partition\n",
        "\n",
        "help(unstructured.partition)"
      ],
      "metadata": {
        "id": "QzVVPXXuXECx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saas_api_key_auth = os.environ.get('saas_api_key_auth')\n",
        "saas_server_url = os.environ.get('saas_server_url')\n",
        "free_api_key_auth = \"TkP5VX7IUEk96NF0Ks2o0l8fGapJGo\"\n",
        "filename = \"/content/BRISQUE.pdf\""
      ],
      "metadata": {
        "id": "RNj4d_FiWI-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = UnstructuredClient(\n",
        "    api_key_auth=free_api_key_auth\n",
        "    #api_key_auth=saas_api_key_auth,\n",
        "    #server_url=saas_server_url,\n",
        ")"
      ],
      "metadata": {
        "id": "N1uP6_Q5WOGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filename, \"rb\") as f:\n",
        "    files=shared.Files(\n",
        "        content=f.read(),\n",
        "        file_name=filename,\n",
        "    )\n",
        "\n",
        "req = shared.PartitionParameters(\n",
        "    files=files,\n",
        "    strategy=\"hi_res\",\n",
        "    hi_res_model_name=\"yolox\",\n",
        "    skip_infer_table_types=[],\n",
        "    pdf_infer_table_structure=True,\n",
        ")\n",
        "\n",
        "try:\n",
        "    resp = client.general.partition(req)\n",
        "    elements = dict_to_elements(resp.elements)\n",
        "except SDKError as e:\n",
        "    print(e)\n",
        "\n",
        "tables = [el for el in elements if el.category == \"Table\"]"
      ],
      "metadata": {
        "id": "JqziQXbxWQ92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = [el for el in elements if el.category == \"Image\"]\n",
        "images"
      ],
      "metadata": {
        "id": "3yspPnALc7Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(images[0]))"
      ],
      "metadata": {
        "id": "xnrRHGNTklQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure_caption = [el for el in elements if el.category == \"FigureCaption\"]\n",
        "print(len(figure_caption))"
      ],
      "metadata": {
        "id": "HKxAZuD3dJj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(figure_caption[15].text)"
      ],
      "metadata": {
        "id": "9AHT1RmMdTh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elements"
      ],
      "metadata": {
        "id": "Blx0EdRacZq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath1 = '/content/Qwen_VL.pdf'\n",
        "filepath2 = '/content/BRISQUE.pdf'"
      ],
      "metadata": {
        "id": "qfGIDgkjsKYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "import pymupdf\n",
        "\n",
        "document1 = PyMuPDFLoader(filepath1, extract_images = True)\n",
        "loading = document1.load()\n",
        "len(loading)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz8FyQR0dfxu",
        "outputId": "03e10eaa-6e69-4dcd-bf7f-cb7d608826b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document2 = PyMuPDFLoader(filepath2, extract_images = True)\n",
        "loading.extend(document2.load())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "FEWLkV_PslG2",
        "outputId": "81c87742-d9b1-4a16-be79-c495cac842e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-474bc5c833dd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdocument2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyMuPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlazy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m_lazy_load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py\u001b[0m in \u001b[0;36mlazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiletype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             yield from [\n\u001b[0m\u001b[1;32m    279\u001b[0m                 Document(\n\u001b[1;32m    280\u001b[0m                     \u001b[0mpage_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 Document(\n\u001b[1;32m    280\u001b[0m                     \u001b[0mpage_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                     \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_images_from_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m                     metadata=dict(\n\u001b[1;32m    283\u001b[0m                         {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py\u001b[0m in \u001b[0;36m_extract_images_from_page\u001b[0;34m(self, doc, page)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 )\n\u001b[1;32m    316\u001b[0m             )\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mextract_from_images_with_rapidocr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py\u001b[0m in \u001b[0;36mextract_from_images_with_rapidocr\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mocr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rapidocr_onnxruntime/main.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img_content, use_det, use_cls, use_rec, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_det\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_add_letterbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mdt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_elapse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_text_det\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdt_boxes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rapidocr_onnxruntime/main.py\u001b[0m in \u001b[0;36mauto_text_det\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     ) -> Tuple[Optional[List[np.ndarray]], float]:\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mdt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_elapse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_det\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdt_boxes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt_boxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rapidocr_onnxruntime/ch_ppocr_v3_det/text_detect.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepro_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mdt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_boxes_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mori_img_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_tag_det_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mori_img_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rapidocr_onnxruntime/utils/infer_engine.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_content)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0minput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_content\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0merror_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(loading))"
      ],
      "metadata": {
        "id": "pYfRqZE3dpo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_title = [el for el in elements if el.category == \"Title\"]"
      ],
      "metadata": {
        "id": "GRcWIBWJcgM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_title"
      ],
      "metadata": {
        "id": "3mzP3Sh3cvsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_data = \"\"\n",
        "for i in range(len(texts_title)):\n",
        "  title_data += texts_title[i].text\n",
        "#print(text)\n",
        "print(len(title_data))"
      ],
      "metadata": {
        "id": "0-Z4_D80cs7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(title_data)"
      ],
      "metadata": {
        "id": "F9E08DgrdF_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tables"
      ],
      "metadata": {
        "id": "TrlgTCvfWXRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tables))"
      ],
      "metadata": {
        "id": "IwcTDzwSWYHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\n",
        "for i in range(len(tables)):\n",
        "  text += tables[i].text\n",
        "#print(text)\n",
        "print(len(text))"
      ],
      "metadata": {
        "id": "UcmyeazPWhDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_raw_into_htmlTable(i,tables = tables):\n",
        "  table_html = tables[i].metadata.text_as_html\n",
        "  # view what the HTML in the metadata field looks like\n",
        "\n",
        "  from io import StringIO\n",
        "  from lxml import etree\n",
        "\n",
        "  parser = etree.XMLParser(remove_blank_text=True)\n",
        "  file_obj = StringIO(table_html)\n",
        "  tree = etree.parse(file_obj, parser)\n",
        "  #print(etree.tostring(tree, pretty_print=True).decode())\n",
        "\n",
        "  # let's display this table\n",
        "\n",
        "  from IPython.core.display import HTML\n",
        "  res = HTML(table_html)\n",
        "  return res\n"
      ],
      "metadata": {
        "id": "IprZQMJHLLz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_html = tables[1].metadata.text_as_html"
      ],
      "metadata": {
        "id": "dqETRsgnWkAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_html"
      ],
      "metadata": {
        "id": "LY_8zmm0Wm8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view what the HTML in the metadata field looks like\n",
        "\n",
        "from io import StringIO\n",
        "from lxml import etree\n",
        "\n",
        "parser = etree.XMLParser(remove_blank_text=True)\n",
        "file_obj = StringIO(table_html)\n",
        "tree = etree.parse(file_obj, parser)\n",
        "print(etree.tostring(tree, pretty_print=True).decode())"
      ],
      "metadata": {
        "id": "vX7LmWeqWpuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's display this table\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "HTML(table_html)"
      ],
      "metadata": {
        "id": "ghC5MIBvWs6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_tables = []\n",
        "for i in range(10):\n",
        "  result = convert_raw_into_htmlTable(i,tables = tables)\n",
        "  html_tables.append(result)"
      ],
      "metadata": {
        "id": "68sxyBZUKx67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= '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'\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=128,temperature=0.7,token='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')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WJdOgzth1Uc",
        "outputId": "8243d074-6b72-4d5a-8081-d519fbaf3127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n",
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! token is not default parameter.\n",
            "                    token was transferred to model_kwargs.\n",
            "                    Please make sure that token is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
        "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "G7pOuU-CNV9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to tables\n",
        "tables = [i.text for i in tables]\n",
        "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
      ],
      "metadata": {
        "id": "kvqWPWn5NLW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_tables[0]"
      ],
      "metadata": {
        "id": "1pGCaVB8ME6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(table_summaries[0])"
      ],
      "metadata": {
        "id": "WXKyxZl_NumZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "image_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "image_to_text(\"/content/1.jpeg\")"
      ],
      "metadata": {
        "id": "iG9XVNL-ROLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_to_text(\"/content/9.png\"))\n",
        "print(image_to_text(\"/content/68.jpeg\"))\n",
        "print(image_to_text(\"/content/67.jpeg\"))\n",
        "print(image_to_text(\"/content/66.jpeg\"))\n",
        "print(image_to_text(\"/content/65.jpeg\"))\n",
        "print(image_to_text(\"/content/64.jpeg\"))"
      ],
      "metadata": {
        "id": "n7UNp4CSR1Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "max_length = 16\n",
        "num_beams = 4\n",
        "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "def predict_step(image_paths):\n",
        "  images = []\n",
        "  for image_path in image_paths:\n",
        "    i_image = Image.open(image_path)\n",
        "    if i_image.mode != \"RGB\":\n",
        "      i_image = i_image.convert(mode=\"RGB\")\n",
        "\n",
        "    images.append(i_image)\n",
        "\n",
        "  pixel_values = feature_extractor(images=images[0], return_tensors=\"pt\").pixel_values\n",
        "  pixel_values = pixel_values.to(device)\n",
        "\n",
        "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
        "\n",
        "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "  preds = [pred.strip() for pred in preds]\n",
        "  return preds\n",
        "\n",
        "\n",
        "print(predict_step(['/content/68.jpeg']))"
      ],
      "metadata": {
        "id": "n2vTpT-sSvLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
        "captioner(\"/content/68.jpeg\")"
      ],
      "metadata": {
        "id": "jel_ihfbYiTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG"
      ],
      "metadata": {
        "id": "BmWQEQhcZOjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQvB-i0zhCYy",
        "outputId": "466b7f26-701f-4d53-8443-ca84b7116c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBFXj6sniIxh",
        "outputId": "e8c134fc-f613-469e-bdcd-0222a825fd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.23.5)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.2.34)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.104)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.5.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.7.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.6.20)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers, langchain_huggingface\n",
            "Successfully installed langchain_huggingface-0.0.3 sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(loading)\n",
        "\n",
        "# Embed\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=HuggingFaceEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "'''retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\"\n",
        "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
        "len(docs)'''\n",
        "\n",
        "#### RETRIEVAL and GENERATION ####\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "F0ZsII-vZOFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"explain methodlogy used in qwen VL\")"
      ],
      "metadata": {
        "id": "v_8n5L0hkjt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt\n",
        "\n",
        "# Chain\n",
        "chain = prompt | llm\n",
        "\n",
        "# Run\n",
        "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})'''"
      ],
      "metadata": {
        "id": "3UHdAef2lr3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-query"
      ],
      "metadata": {
        "id": "x4kz5epXyXpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Multi Query: Different Perspectives\n",
        "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "gQOZ_tBXwu7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_queries"
      ],
      "metadata": {
        "id": "cgMIXJfTy9s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# Retrieve\n",
        "question = \"Explain the architecture and training procedure used in Qwen VL\"\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "docs = retrieval_chain.invoke({\"question\":question})\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "DqMuSAwWw5jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "cDb2wdpry0Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "qkM-QeBwxVUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decomposition"
      ],
      "metadata": {
        "id": "5EqeuaHxyRv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Decomposition\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Chain\n",
        "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "# Run\n",
        "question = \"Explain the architecture and training procedure used in Qwen VL\"\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "Zr_rEW0FyL47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions"
      ],
      "metadata": {
        "id": "o_l5BPriymb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "template = \"\"\"Here is the question you need to answer:\n",
        "\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question:\n",
        "\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
        "\"\"\"\n",
        "\n",
        "decomposition_prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "7ugAg9fyzbL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_qa_pair(question, answer):\n",
        "    \"\"\"Format Q and A pair\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "\n",
        "q_a_pairs = \"\"\n",
        "for q in questions:\n",
        "\n",
        "    rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\"),\n",
        "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
        "    | decomposition_prompt\n",
        "    | llm\n",
        "    | StrOutputParser())\n",
        "\n",
        "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
        "    q_a_pair = format_qa_pair(q,answer)\n",
        "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
      ],
      "metadata": {
        "id": "3VhwX8zXzfBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "id": "NNOYGjlDznvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-representation indexing"
      ],
      "metadata": {
        "id": "fxziZCMvqZBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chain = (\n",
        "    {\"doc\": lambda x: x.page_content}\n",
        "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "summaries = chain.batch(loading, {\"max_concurrency\": 5})"
      ],
      "metadata": {
        "id": "k-cQceBCzxS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summaries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBsPLNqbxB_T",
        "outputId": "61dd5a6b-1015-4754-d7da-db31b0404313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nThe document introduces the Qwen-VL series, a set of large-scale vision-language models (LVLMs) developed by Alibaba Group researchers. The models are designed to perceive and understand both texts and images. The team enhanced the Qwen-LM foundation by adding visual capacity using a visual receptor, input-output interface, 3-stage training pipeline, and a multilingual multimodal cleaned corpus. The models go beyond traditional image description and question answering by implementing grounding and text-reading abilities. The Qwen-VL and Qwen-VL-Chat models set new records on various visual-centric benchmarks and real-world dialog benchmarks, respectively. The researchers made all models public to encourage further research. The document includes a figure showing the state-of-the-art performance of Qwen-VL compared to other generalist models.', '\\n13\\n14\\nIn this work, we introduce the Qwen-VL series, a set of large-scale\\nvision-language models (LVLMs) designed to perceive and\\nunderstand both texts and images. Starting from the Qwen-LM\\nCan you describe the person in the picture?\\nas a foundation, we assign it visual capacity by meticulously designed\\n(i) visual receptor, (ii) input-output interface, (iii) 3-stage training\\nThe person in the picture is wearing a black coat and looks quite\\npipeline, and (iv) multilingual multimodal cleaned corpus. Beyond\\nrelaxed.\\nthe conventional description and question-answering, we inject the\\ngrounding ability into Qwen-VLs by importing fine-grained image-caption-box\\npairs. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new\\nrecords on a broad range of visual-centric benchmarks (e.g., image captioning,\\nquestion answering, visual grounding) under different settings (e.g., zero-shot,\\nfew-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned\\nQwen-VL-Chat also demonstrates conspicuous superiority compared to\\nexisting vision-language chatbots. All models will be made public to facilitate\\nfuture research.\\n\\nThe text introduces the Qwen-VL and Owen-VL series, which are sets of large-scale vision-language models designed to perceive and understand both texts and images. The models are built on foundational LVMs and are enhanced with visual capacity through visual receptors, input-output interfaces, 3-stage training pipelines, and multilingual multimodal cleaned corpora. The grounding ability is added to the models through fine-grained image-caption-box pairs. The models have set new records on various visual-centric benchmarks and demonstrate superiority on real-world dialog benchmarks compared to existing vision-language chatbots. All models will be made public for future research. The text also includes examples of qualitative outputs from the models.', \"of the position information, the adapter includes position-aware learnable weights to attend to the position\\ninformation of the input images.\\n\\nThis document discusses the Qwen-VL series, a new set of highly performant and versatile vision-language foundation models based on Qwen-7B. These models have been enhanced with visual capacity through the addition of a new visual receptor, which includes a language-aligned visual encoder and a position-aware adapter. The Qwen-VL models are capable of understanding and generating responses based on visual inputs, and can perform various vision-language tasks such as image captioning, question answering, and visual grounding. They achieve leading performance on vision-centric understanding benchmarks and support multiple languages and multiple images as inputs. The models' fine-grained visual understanding ability is competitive with existing vision-language generalists. The document also provides details on the model architecture, which includes a large language model, a visual encoder, and a position-aware vision-language adapter. The visual encoder uses the Vision Transformer architecture and the position-aware adapter compresses the image features and attends to the position information of the input images.\", \"Text Input: The text input is processed by the large language model, which generates responses based on the\\nimage features, bounding box inputs, and the text itself.\\nOutput: The output is a textual response, which can be a question answer, a region description, or a caption.\\n\\nThis document describes the Qwen-VL model, which is designed for fine-grained image comprehension. The model uses a vision encoder and adapter to extract image features, which are then compressed and fed into a large language model. To maintain positional information during compression, 2D absolute positional encodings are incorporated into the cross-attention mechanism's query-key pairs. The model also handles bounding box inputs and outputs, which require accurate understanding and generation of region descriptions. The text input is processed by the large language model, and the output is a textual response. The document provides details of the Qwen-VL model's parameters, including the vision encoder, VL adapter, and large language model, as well as the training pipeline.\", '2016). The input resolution is set to 384 × 384 and the maximum learning rate is 1e−5 with a batch size of\\n12800 for the image-text pairs, and the entire second stage of multi-task pre-training lasts for 100,000 steps.\\n3.3\\nInstruction Fine-Tuning\\nIn the final stage of instruction fine-tuning training, we fine-tune the entire model on a large-scale, high-quality\\ninstruction dataset. We use a learning rate schedule to gradually decrease the learning rate from the initial\\nvalue of 1e−5 to 1e−6 over the course of 25 epochs. The batch size is set to 4096 and the entire fine-tuning\\nprocess lasts for 100 epochs. The entire Qwen-VL model is fine-tuned in this stage. The convergence curve\\nof this stage is shown in Figure 7.\\n\\nThe Qwen-VL model is a multimodal model for vision and language processing. It consists of a large language model, a vision encoder, and a vision-language adapter. The training process of the Qwen-VL model includes two stages of pre-training and a final stage of instruction fine-tuning. In the first stage of pre-training, a large-scale, weakly labeled, web-crawled set of image-text pairs is used. The pre-training dataset is composed of several publicly accessible sources and some in-house data, which is cleaned to remove certain patterns. The input images are resized to 224 × 224 and the training objective is to minimize the cross-entropy of the text tokens. The second stage of multi-task pre-training introduces high-quality and fine-grained VL annotation data with a larger input resolution and interleaved image-text data. The model is trained on 7 tasks simultaneously, including text generation, captioning, VQA, grounding, reference grounding, and grounded captioning duality tasks. The input resolution is set to 384 × 384 and the training objective is to minimize the loss of all tasks. In the final stage of instruction fine-tuning, the entire model is fine-tuned on a large-scale,', 'Image Captioning Results\\nThe Qwen-VL model achieves a score of 41.1 on RGB-D Sentence BLEU and 40.5 on BLEU-4, 37.2 on METEOR,\\nand 36.2 on CIDEr. The results demonstrate that the model has a reasonable performance in generating\\ndescriptions for images, especially considering the challenging nature of the RGB-D dataset.\\n7\\nVQA Results\\nThe Qwen-VL model achieves a score of 65.2 on Split-Val and 64.7 on HS-Val on the LVIS dataset, which is\\nconsiderably higher than the results reported by other models in the literature (e.g., ViLBERT, LXMERT, and\\nALBEF). The results indicate that the model has a strong ability to understand and reason about images.\\n8\\nImage Captioning and VQA Datasets\\nThe Qwen-VL model is evaluated on a variety of datasets, including RGB-D, LVIS, CC3M, and OpenImages.\\nThese datasets cover a wide range of image types and contain a large number of samples, ensuring a thorough\\nassessment of the model’s performance.\\n9\\nZero-Shot Transfer and Generalization\\nThe Qwen-VL model demonstrates impressive zero-shot transfer and generalization abilities on downstream tasks,\\nsuch as visual grounding, semantic textual similarity, and question answering. Specifically, the model achieves\\nstate-of-the-art results on the Open-Domain Question Answering benchmark and the Visual Question Answering\\nchallenge.\\n10\\nHuman-AI Interaction and Dialogue\\nThe Qwen-VL model is also evaluated on human-AI interaction tasks, such as multi-turn dialogues, text-based\\nconversations, and story understanding. The results indicate that the model effectively transfers its\\ninstruction following and dialogue capabilities to human-AI interaction tasks.\\n11\\nConclusion\\nIn conclusion, the Qwen-VL model is a strong multi-modal model that demonstrates a strong ability to understand\\nand reason about images, as well as a strong ability to generate descriptive captions and answer questions\\nrelated to images. The model’s impressive zero-shot transfer and generalization abilities make', 'performance on these benchmarks is shown in Table 6. Our models significantly outperform previous generalist\\nmodels and recent LVLMs on most benchmarks.\\n4.4\\nZero-shot Object Detection and Localization\\nWe also evaluate our models’ zero-shot object detection and localization ability on COCO (Lin et al., 2014) and\\nObjectQuest (Gao et al., 2019) datasets. The performance is reported in Table 7. As the results show, our models\\nperform well on these datasets, and the performance is even comparable to some models with more parameters\\nand fine-tuning. It’s worth noting that our models also show strong zero-shot performance on several other\\ndatasets including LVIS (Gupta et al., 2019), OpenImage (Kuznietsov et al., 2017), and Visual Genome (Krishna et al.,\\n2017).\\n4.5\\nZero-shot Entity Recognition and Relation Extraction\\nFinally, we evaluate our models’ ability to understand entities and extract relationships between them in a\\nzero-shot setting on the ConceptNet (Speer et al., 2017) dataset. The performance is reported in Table 8. Our\\nmodels perform well on this dataset, and the performance is even comparable to some models with more\\nparameters and fine-tuning. It’s worth noting that our models also show strong zero-shot performance on several\\nother datasets including AIDA (Hoffman et al., 2011), WordNet (Miller, 1995), and NERD (Chang et al., 2018).\\n\\nThis document discusses the performance of various models on tasks related to visual understanding, including image captioning, general visual question answering, text-oriented visual question answering, refer expression comprehension, zero-shot object detection and localization, and zero-shot entity recognition and relation extraction. The document highlights the superior performance of the Qwen-VL and Qwen-VL-Chat models compared to previous models in these tasks, particularly in zero-shot settings. The models were evaluated on several benchmarks, and the results showed significant improvements in most cases.', '\\n0\\n1\\n2\\n4\\n8\\n0\\n1\\n2\\n8\\n0\\n1\\n2\\n4\\n8\\n0\\n1\\n2\\n8\\n0\\n1\\n2\\n4\\n\\nPlease note that the document provided is a summary of the research findings from a paper. The actual paper may contain more detailed information and context. The document describes the results of various models on Text-oriented Question Answering (TextVQA, DocVQA, ChartQA, AI2D, OCR-VQA), Referring Expression Comprehension (RefCOCO, RefCOCO+, RefCOCOg, GRIT, val, test-A test-B), and few-shot learning (OKVQA, Vizwiz, TextVQA, Flickr30k) tasks. The models are categorized as generalist or specialist models based on their architecture and training methodology. The document also compares the performance of these models against each other and against previous state-of-the-art models. The research findings suggest that the models developed in the study obtain top-tier results on all benchmarks, and exhibit satisfactory few-shot learning ability.', \"as weak understanding of long-term dependencies and inability to handle complex instructions. To address\\nthese limitations, our work focuses on improving the instruction-following capability of vision-language\\nmodels, enabling them to better understand and execute complex user instructions.\\n\\nThis document discusses the results of an evaluation of a model called Qwen-VL-Chat on various instruction-following benchmarks, including TouchStone, SEED-Bench, and MME. The model outperformed other LVLMs (Large Vision-Language Models) on all three datasets, indicating better understanding and answering of diverse user instructions. The evaluation also found that Qwen-VL-Chat's visual capabilities could be effectively transferred to video tasks by sampling a few frames. The document also mentions related work in the field of vision-language learning and representation models, and the limitations of previous models, which the current work aims to address by improving instruction-following capability.\", 'OpenAI. DALL-E: creating images from text. 2023.\\nQiang Qwen, Jing Tao, and Xiaohui Xu. Qwen-VL: A large-scale multilingual vision-language model. In Proc. of\\nIJCAI, 2023.\\nXiaohui Xu, Jing Tao, and Qiang Qwen. Visual grounding with multilingual pretraining. In Proc. of CVPR, 2022.\\n\\nThis document discusses the advancements in large vision-language models (LVLMs) and their capabilities, particularly in areas of instruction following, generalization, and in-context abilities. Researchers have been developing more powerful LVLMs based on large language models (LLMs), using techniques like Q-Former for alignment, visual instruction tuning, document understanding, and visual grounding abilities. The document also introduces the Qwen-VL series, a multilingual vision-language model that outperforms similar models across various benchmarks and supports multimodal research. Future work includes integrating more modalities, scaling up the model size, and expanding its prowess in multi-modal generation.', '\\nThis document contains information about various research papers and datasets related to image-text pairing and multimodal models. The first set of papers and datasets are related to the Coyo-700m dataset, which was created by Minwoo Byeon et al. Conceptual 12m is a paper that discusses pushing web-scale image-text pre-training to recognize long-tail visual concepts. Shikra, Uniter, and Pali are multimodal models that were developed for referential dialogue, universal image-text representation learning, and scaling up a multilingual vision and language model, respectively. Pali-x is a follow-up to Pali, focusing on scaling up the model even further. Microsoft coco captions and Uniter are image-text pairing datasets used in these models. The document also mentions a few other papers and datasets related to image-text pairing and multimodal models. An image is worth 16x16 words is a paper about Transformers for image recognition at scale. Coarse-to-fine vision-language pre-training with fusion in the backbone is another paper that discusses a comprehensive evaluation benchmark for multimodal large language models. Datacomp is a search for the next generation of multimodal datasets. Llama-adapter v2 is a parameter-efficient visual instruction model. Imagebind is a paper about one embedding space to bind them all. Making the v in vqa matter is a paper that elevates the role of image understanding in visual question answering. Grit is a general robust image task benchmark. Vizwiz grand challenge is a challenge to answer visual questions from blind people. Puppeteer is a popular automation library for controlling headless Chrome or Chromium browsers. The authors of the papers and the years of publication are also provided.', '\\nThis document contains references to various research papers related to multimodal machine learning and computer vision. The papers cover a range of topics including multitask learning, aligning perception with language models, real-world visual reasoning and question answering, open-domain visual grounding, document understanding, and multimodal representation learning. Some papers propose new datasets and models, while others focus on improving existing methods or benchmarking new approaches. The authors of these papers include researchers from various institutions and organizations, such as Microsoft Research, Google Research, and Carnegie Mellon University. The papers were published in various conferences and journals, including ICCV, ICML, ECCV, and CVPR. Some papers are from as early as 2014, while others are from more recent years.', '\\nThis document contains a list of research papers and reports related to multimodal pretraining, vision-language tasks, and large language models. The authors of these papers include Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick, Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi, Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, Enamul Hoque, Minesh Mathew, Dimosthenis Karatzas, CV Jawahar, Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.\\n\\nThe papers cover topics such as M6: A Chinese multimodal pretrainer, Microsoft COCO: Common objects in context, Visual instruction tuning, Unified-IO: A unified model for vision, language, and multi-modal tasks, Learn to explain: Multimodal reasoning via thought chains for science question answering, Generation and comprehension of unambiguous object descriptions, Ok-VQA: A visual question answering benchmark requiring external knowledge, ChartQA: A benchmark for question answering about charts with visual and logical reasoning, DocVQA: A dataset for VQA on document images, OCR-VQA: Visual question answering', '\\nThis document lists various research papers related to computer vision and natural language processing (NLP). The papers cover various aspects, including language and vision alignment, pre-training of visual-linguistic representations, generative pre-training in multimodality, image description evaluation, unifying architectures and tasks through sequence-to-sequence learning, and exploring one general representation model toward unlimited modalities. Some papers also discuss specific models like Cider, Ofa, One-peace, Image as a foreign language, Chinese clip, mplug-docowl, mplug-owl, Florence, Multi-grained vision language pre-training, Lit, and Video-llama. The authors of these papers include Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela, Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang, Ramakrishna Vedantam, C Lawrence Zitnick, Devi Parikh, Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, Lijuan Wang, Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, and many others.', '\\nThese papers discuss advancements in the field of vision-language models. The first paper by Zhang et al. introduces Vinvl, a method for revisiting visual representations in vision-language models. They propose a new pre-training strategy to learn visual features that are beneficial for vision-language tasks.\\n\\nThe second paper by Zhao et al. presents Bubogpt, a method for enabling visual grounding in multi-modal large language models. They propose a new framework that can handle both visual and textual inputs and generate grounded descriptions of images.\\n\\nThe third paper by Zhu et al. discusses Minigpt-4, which enhances vision-language understanding by integrating advanced large language models. They propose a new architecture that can handle various vision-language tasks, such as image captioning, visual question answering, and visual grounding.\\n\\nThe fourth paper by Zhu et al. introduces Uni-perceiver, a pre-training method for generic perception tasks. They propose a unified architecture that can handle both zero-shot and few-shot tasks and achieve state-of-the-art performance on various benchmarks.', '2. Extracting the image content of each page.\\n3. Converting text to plain text format.\\n4. Combining all texts and their bounding boxes of all pages to build a dataset.\\n5. Combining all images and their bounding boxes of all pages to build a dataset.\\n\\nThe document describes the process of preparing and cleaning various datasets for use in machine learning models. The datasets include image-text pairs, VQA (Visual Question Answering), grounding, and OCR (Optical Character Recognition). The image-text pairs dataset is collected from multiple sources and cleaned by removing pairs with specific characteristics. The VQA dataset uses maximum confidence for answer selection. The grounding dataset is cleaned by removing recursive grounding box labels. The OCR dataset is generated synthetically using Synthdog. The PDF data is pre-processed using PyMuPDF to extract text and images.', \"\\n21100.90\\nMerrill LynchInvestmentInterest\\n475.11\\n1,857.58\\n521100.9\\n5475.1\\n01475.1\\n35.0\\n21100.95\\nPersonal Income\\n31,343.78\\n14,584.40\\n521100.95\\n31,343.7\\nPersonal Expenses\\n21,620.82\\n13,511.69\\n521100.95\\n17,620.8\\nTotal\\n13,448.38\\n10,672.71\\n521100.95\\n2,981.07\\n\\nThe document is a monthly statement summary of the Pinal County Treasurer's account from April 1, 2018, to April 30, 2018. It includes various transactions such as real estate taxes, personal property taxes, contributions, and investment interests. The document also shows the beginning and ending balances of the account during this period. The four Korean novel types mentioned in the beginning of the document - life, spirit, recorded, and OIC novels - do not have a clear connection to the monthly statement summary.\", 'B.1\\nMulti-Task Pre-training Data Format\\n[Input]\\n[Page Image]\\n[Page Text]\\n[Text OCR]\\n[Text OCR with Unicode]\\n[Text OCR with Unicode in Latin Extended-A and Latin Extended-B]\\n[Text OCR with Unicode in Private Use Area (PUA)]\\n[Task 1: Text Classification]\\n[Task 2: Entity Extraction]\\n[Task 3: Relation Extraction]\\n[Task 4: Named Entity Recognition]\\n[Task 5: Part-of-Speech Tagging]\\n[Task 6: Sentiment Analysis]\\n[Task 7: Named Entity Coreference Resolution]\\n\\nThis document describes a process for pre-processing HTML webpages for multi-task pre-training data. The process includes extracting all texts from each webpage, rendering the pages as image files, removing certain images based on size, character count, and Unicode character blocks, and using Puppeteer to render the HTML pages. The pre-processed data includes the page image, page text, text OCR, text OCR with Unicode, and text OCR with Unicode in specific blocks for each of the seven tasks in the multi-task pre-training: text classification, entity extraction, relation extraction, named entity recognition, part-of-speech tagging, sentiment analysis, and named entity coreference resolution.', '<im_start>user\\nPicture id: 0 Picture 1: <img>vg/VG_100K_2/649.jpg</img>What is the sign in the picture?<im_end>\\n<im_start>assistant\\nThe sign is a road closure with an orange rhombus.<im_end>\\n<im_start>user\\nPicture id: 1 Picture 2: <img>vg/VG_100K_2/823.jpg</img>What is the color of the car in the picture?<im_end>\\n<im_start>assistant\\nThe car is red.<im_end>\\n<im_start>user\\nPicture id: 2 Picture 3: <img>vg/VG_100K_2/731.jpg</img>What is the animal in the picture?<im_end>\\n<im_start>assistant\\nThe animal is a giraffe.<im_end>\\n<im_start>user\\nPicture id: 3 Picture 4: <img>vg/VG_100K_2/428.jpg</img>What is the fruit in the picture?<im_end>\\n<im_start>assistant\\nThe fruit is an apple.<im_end>\\n<im_start>user\\nPicture id: 4 Picture 5: <img>vg/VG_100K_2/694.jpg</img>What is the sign in the picture?<im_end>\\n<im_start>assistant\\nThe sign is a warning sign with a triangle and an exclamation mark.<im_end>\\n<im_start>user\\nPicture id: 5 Picture 6: <img>vg/VG_100K_2/177.jpg</img>What is the color of the house in the picture?<im_end>\\n<im_start>assistant\\nThe house is brown.<im_end>\\n<im_start>user\\nPicture id: 6 Picture 7: <img>vg/VG_100K_2/264.jpg</img>What is the animal in', \"| Table 9: Evaluation benchmarks and corresponding metrics |\\n| Evaluation Benchmark | Metrics |\\n| ---------------------| -------- |\\n| GLUE (SuperGLUE)     | Accuracy |\\n| SQuAD                | EM, F1    |\\n| MNLI                  | Accuracy |\\n| RACE                 | Accuracy |\\n| LAMA                 | Accuracy |\\n| BERTScore            | Pearson |\\n| ROUGE-L              | F1 score |\\n| T5                    | BLEU,\\n   ROUGE-L             |\\n| DPRC                 | Precision, Recall, F1 |\\n| MRC                  | F1 score |\\n| Sentiment Analysis    | Accuracy |\\n| Question Answering   | Accuracy |\\n| Text Classification  | Accuracy |\\n\\nThe document provides the training hyperparameters for a model named Qwen-VL. In the first pre-training stage, the model is trained with a resolution of 224x224, ViT sequence length of 256, and LLM sequence length of 512. The optimizer used is AdamW with a peak learning rate of 2e-4, minimum learning rate of 1e-6, and a cosine learning rate decay. The model uses a batch size of 30720 and lasts for 50,000 steps. In the second pre-training stage, the model's input resolution is increased to 448x448, and the large language model is unlocked. The optimizer, learning rate schedule, and batch size remain the same. The model parallelism techniques are used for ViT and LLM. The evaluation benchmarks used include GLUE (SuperGLUE), SQuAD, MNLI, RACE, LAMA, BERTScore, ROUGE-L, T5, DPRC, MRC, Sentiment Analysis, Question Answering, and Text Classification, with corresponding metrics such as accuracy, F1 score, and Pearson correlation.\", '23\\n25\\n27\\n29\\n31\\n33\\n35\\n37\\nNumber of Queries\\nfind that the optimal number of learnable queries is around 25, which balances the trade-off between\\nvisual information loss and convergence difficulty.\\nE.3\\nPerformance on the Validation Set with Different Pre-trained Models\\nThe model is evaluated on the validation set with different pre-trained models to understand the performance\\nimprovement. The results are summarized in Table 11. We observe that the performance improves significantly\\nwhen the pre-trained models are fine-tuned on the downstream tasks. The most significant improvement is\\nobserved in the VQA tasks, followed by the refer grounding tasks. The performance on the other tasks also\\nshows improvement, but the magnitude of improvement is relatively smaller.\\nE.4\\nAblation Study on the Effectiveness of the Vision-Language Adapter\\nAn ablation experiment is conducted to understand the effectiveness of the vision-language adapter. The results\\nare summarized in Table 12. We find that the vision-language adapter significantly improves the performance\\non the downstream tasks, especially the VQA tasks. The improvement in the VQA tasks is consistent with the\\nfindings from previous studies. The improvement in the refer grounding tasks is also significant, but not as\\nlarge as in the VQA tasks. The performance on the other tasks also shows improvement, but the magnitude of\\nimprovement is relatively smaller.\\nE.5\\nPerformance on the Test Set with Different Pre-trained Models\\nThe model is evaluated on the test set with different pre-trained models to understand the generalization\\nperformance. The results are summarized in Table 13. We observe that the model performs well on the test set\\nwith different pre-trained models, indicating good generalization performance. The performance on the VQA tasks\\nis particularly impressive, with the model achieving state-of-the-art results on several benchmarks.\\nE.6\\nComparison with Other State-of-the-Art Models\\nThe model is compared with other state-of-the-art models on several benchmarks to understand its performance.\\nThe results are summarized in Table 14. We observe that the model outperforms the other models on several\\nbenchmarks, indicating its superior performance.', '23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\n62\\n63\\n64\\n65\\n66\\n67\\n68\\n69\\n70\\nSteps\\n2.65\\n2.70\\n2.75\\n2.80\\n2.85\\n2.90\\n2.95\\nLoss\\nL144\\nL256\\nL400\\nL1000\\nL1500\\nL2000\\nL2500\\nL3000\\nL3500\\nL4000\\nL4500\\n\\nThis document discusses the use of a vision-language adapter in a model called Qwen-VL. The adapter compresses the visual feature sequence to a fixed length using a certain number of queries. The document suggests that using fewer queries at the beginning of training can lead to a lower initial loss but may cause slower convergence with a large number of queries. The document also mentions that using a high-resolution Vision Transformer in the model increases computational cost, and reducing this cost by using Window Attention in most layers instead of Global Attention did not lead to significant improvements in training speed or convergence, so Vanilla Attention was used instead. The document provides a visualization of the training loss when using different compressed feature lengths of the vision-language adapter and compares the training speed of Window Attention and Global Attention for different input image resolutions.', \"\\nFigure 9: Comparison of Training Speed and Loss of Window Attention and Global Attention\\n\\nThis document discusses the performance of a model named Qwen-VL compared to other text-only LLMs like LLaMA-7B, Baichuan-7B, ChatGLM2-6B, InternLM-7B, and Qwen-7B. It mentions that Qwen-VL uses an intermediate checkpoint of Qwen-7B as its LLM initialization. The document also compares the performance of Qwen-VL on pure-text tasks with the open-source LLM. The model, Qwen-VL, is able to utilize visual and language-related data, as well as pure-text data, during the multi-task training and SFT stages to prevent any compromise in pure-text ability. The document also mentions that Window Attention takes longer to train and has a similar loss value to a model with 448 × 448 resolution input, but it's not used due to its slow training speed. The document includes a figure showing the comparison of training speed and loss for Window Attention and Global Attention.\", 'In contrast, the RoBERTa model shows a significant drop in performance, particularly in the context of\\nmulti-task learning, as can be seen in Table 12. However, when focusing on the pure text task, the RoBERTa\\nmodel still performs comparably to the Qwen-VL model.\\n\\nThis document suggests that two models, Qwen-VL and RoBERTa, were tested for their text comprehension capabilities. The Qwen-VL model maintained its performance and even improved slightly after multi-task training. Conversely, the RoBERTa model experienced a significant drop in performance during multi-task learning, but its pure text task capability remained comparable to the Qwen-VL model.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.storage import InMemoryByteStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"summaries\",\n",
        "                     embedding_function=HuggingFaceEmbeddings())\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in loading]\n",
        "\n",
        "# Docs linked to summaries\n",
        "summary_docs = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(summaries)\n",
        "]\n",
        "\n",
        "# Add\n",
        "retriever.vectorstore.add_documents(summary_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, loading)))"
      ],
      "metadata": {
        "id": "GCACnpLKq-sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"explain the architecture and training procedure used in Qwen\"\n",
        "sub_docs = vectorstore.similarity_search(query,k=1)\n",
        "sub_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Beppht_4rB-8",
        "outputId": "7286fd02-b464-44e2-cca4-18b579c9d98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'doc_id': 'fff91aab-ff89-4e61-89d5-289195707390'}, page_content=\"\\nFigure 9: Comparison of Training Speed and Loss of Window Attention and Global Attention\\n\\nThis document discusses the performance of a model named Qwen-VL compared to other text-only LLMs like LLaMA-7B, Baichuan-7B, ChatGLM2-6B, InternLM-7B, and Qwen-7B. It mentions that Qwen-VL uses an intermediate checkpoint of Qwen-7B as its LLM initialization. The document also compares the performance of Qwen-VL on pure-text tasks with the open-source LLM. The model, Qwen-VL, is able to utilize visual and language-related data, as well as pure-text data, during the multi-task training and SFT stages to prevent any compromise in pure-text ability. The document also mentions that Window Attention takes longer to train and has a similar loss value to a model with 448 × 448 resolution input, but it's not used due to its slow training speed. The document includes a figure showing the comparison of training speed and loss for Window Attention and Global Attention.\")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
        "retrieved_docs[0].page_content[0:500]"
      ],
      "metadata": {
        "id": "wfIbTYjFrCpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-ranking"
      ],
      "metadata": {
        "id": "1BZh25oQ0w7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(loading)\n",
        "\n",
        "# Index\n",
        "from langchain_community.vectorstores import Chroma\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "uJI8YGuv0wkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(retriever)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAEVuIcp1Y43",
        "outputId": "6a87a2f0-eee0-4610-a56c-19e528860199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tags=['Chroma', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7fb3c1d57910>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# RAG-Fusion\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "Ru6ymk901kSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
        "            doc_str = dumps(doc)\n",
        "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # Retrieve the current score of the document, if any\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "\n",
        "\n",
        "question = \"What is architecture and training method for Qwen?\"\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2YyBnVv1vQa",
        "outputId": "7d15c511-3f20-4a72-cacb-9f581d1c4d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os1c71yf2Jcz",
        "outputId": "e5bfab1f-1f93-4013-f4b4-512629d957aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(Document(metadata={'doc_id': 'df3327af-a3bf-4bb0-9b11-c15c379ee53e'}, page_content='23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\n62\\n63\\n64\\n65\\n66\\n67\\n68\\n69\\n70\\nSteps\\n2.65\\n2.70\\n2.75\\n2.80\\n2.85\\n2.90\\n2.95\\nLoss\\nL144\\nL256\\nL400\\nL1000\\nL1500\\nL2000\\nL2500\\nL3000\\nL3500\\nL4000\\nL4500\\n\\nThis document discusses the use of a vision-language adapter in a model called Qwen-VL. The adapter compresses the visual feature sequence to a fixed length using a certain number of queries. The document suggests that using fewer queries at the beginning of training can lead to a lower initial loss but may cause slower convergence with a large number of queries. The document also mentions that using a high-resolution Vision Transformer in the model increases computational cost, and reducing this cost by using Window Attention in most layers instead of Global Attention did not lead to significant improvements in training speed or convergence, so Vanilla Attention was used instead. The document provides a visualization of the training loss when using different compressed feature lengths of the vision-language adapter and compares the training speed of Window Attention and Global Attention for different input image resolutions.'), 0.04838709677419355), (Document(metadata={'doc_id': '93e2fd96-f0f0-4e33-ad00-96385c6b56b3'}, page_content='23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\n62\\n63\\n64\\n65\\n66\\n67\\n68\\n69\\n70\\nSteps\\n2.65\\n2.70\\n2.75\\n2.80\\n2.85\\n2.90\\n2.95\\nLoss\\nL144\\nL256\\nL400\\nL1000\\nL1500\\nL2000\\nL2500\\nL3000\\nL3500\\nL4000\\nL4500\\n\\nThis document discusses the use of a vision-language adapter in a model called Qwen-VL. The adapter compresses the visual feature sequence to a fixed length using a certain number of queries. The document suggests that using fewer queries at the beginning of training can lead to a lower initial loss but may cause slower convergence with a large number of queries. The document also mentions that using a high-resolution Vision Transformer in the model increases computational cost, and reducing this cost by using Window Attention in most layers instead of Global Attention did not lead to significant improvements in training speed or convergence, so Vanilla Attention was used instead. The document provides a visualization of the training loss when using different compressed feature lengths of the vision-language adapter and compares the training speed of Window Attention and Global Attention for different input image resolutions.'), 0.047619047619047616), (Document(metadata={'doc_id': '28b49818-3337-4f7e-ae7c-4076a696be19'}, page_content=\"\\nFigure 9: Comparison of Training Speed and Loss of Window Attention and Global Attention\\n\\nThis document discusses the performance of a model named Qwen-VL compared to other text-only LLMs like LLaMA-7B, Baichuan-7B, ChatGLM2-6B, InternLM-7B, and Qwen-7B. It mentions that Qwen-VL uses an intermediate checkpoint of Qwen-7B as its LLM initialization. The document also compares the performance of Qwen-VL on pure-text tasks with the open-source LLM. The model, Qwen-VL, is able to utilize visual and language-related data, as well as pure-text data, during the multi-task training and SFT stages to prevent any compromise in pure-text ability. The document also mentions that Window Attention takes longer to train and has a similar loss value to a model with 448 × 448 resolution input, but it's not used due to its slow training speed. The document includes a figure showing the comparison of training speed and loss for Window Attention and Global Attention.\"), 0.03333333333333333), (Document(metadata={'doc_id': 'fff91aab-ff89-4e61-89d5-289195707390'}, page_content=\"\\nFigure 9: Comparison of Training Speed and Loss of Window Attention and Global Attention\\n\\nThis document discusses the performance of a model named Qwen-VL compared to other text-only LLMs like LLaMA-7B, Baichuan-7B, ChatGLM2-6B, InternLM-7B, and Qwen-7B. It mentions that Qwen-VL uses an intermediate checkpoint of Qwen-7B as its LLM initialization. The document also compares the performance of Qwen-VL on pure-text tasks with the open-source LLM. The model, Qwen-VL, is able to utilize visual and language-related data, as well as pure-text data, during the multi-task training and SFT stages to prevent any compromise in pure-text ability. The document also mentions that Window Attention takes longer to train and has a similar loss value to a model with 448 × 448 resolution input, but it's not used due to its slow training speed. The document includes a figure showing the comparison of training speed and loss for Window Attention and Global Attention.\"), 0.03278688524590164), (Document(metadata={'doc_id': '885c3d13-eb17-407b-8f57-dbf61234dc6c'}, page_content='<im_start>user\\nPicture id: 0 Picture 1: <img>vg/VG_100K_2/649.jpg</img>What is the sign in the picture?<im_end>\\n<im_start>assistant\\nThe sign is a road closure with an orange rhombus.<im_end>\\n<im_start>user\\nPicture id: 1 Picture 2: <img>vg/VG_100K_2/823.jpg</img>What is the color of the car in the picture?<im_end>\\n<im_start>assistant\\nThe car is red.<im_end>\\n<im_start>user\\nPicture id: 2 Picture 3: <img>vg/VG_100K_2/731.jpg</img>What is the animal in the picture?<im_end>\\n<im_start>assistant\\nThe animal is a giraffe.<im_end>\\n<im_start>user\\nPicture id: 3 Picture 4: <img>vg/VG_100K_2/428.jpg</img>What is the fruit in the picture?<im_end>\\n<im_start>assistant\\nThe fruit is an apple.<im_end>\\n<im_start>user\\nPicture id: 4 Picture 5: <img>vg/VG_100K_2/694.jpg</img>What is the sign in the picture?<im_end>\\n<im_start>assistant\\nThe sign is a warning sign with a triangle and an exclamation mark.<im_end>\\n<im_start>user\\nPicture id: 5 Picture 6: <img>vg/VG_100K_2/177.jpg</img>What is the color of the house in the picture?<im_end>\\n<im_start>assistant\\nThe house is brown.<im_end>\\n<im_start>user\\nPicture id: 6 Picture 7: <img>vg/VG_100K_2/264.jpg</img>What is the animal in'), 0.016666666666666666), (Document(metadata={'doc_id': '19278a56-079e-4303-aacf-fecf53c99d50'}, page_content='\\n0\\n1\\n2\\n4\\n8\\n0\\n1\\n2\\n8\\n0\\n1\\n2\\n4\\n8\\n0\\n1\\n2\\n8\\n0\\n1\\n2\\n4\\n\\nPlease note that the document provided is a summary of the research findings from a paper. The actual paper may contain more detailed information and context. The document describes the results of various models on Text-oriented Question Answering (TextVQA, DocVQA, ChartQA, AI2D, OCR-VQA), Referring Expression Comprehension (RefCOCO, RefCOCO+, RefCOCOg, GRIT, val, test-A test-B), and few-shot learning (OKVQA, Vizwiz, TextVQA, Flickr30k) tasks. The models are categorized as generalist or specialist models based on their architecture and training methodology. The document also compares the performance of these models against each other and against previous state-of-the-art models. The research findings suggest that the models developed in the study obtain top-tier results on all benchmarks, and exhibit satisfactory few-shot learning ability.'), 0.016666666666666666), (Document(metadata={'doc_id': '92c69986-a955-4d6d-8bcd-23aa9850d01b'}, page_content=\"| Table 9: Evaluation benchmarks and corresponding metrics |\\n| Evaluation Benchmark | Metrics |\\n| ---------------------| -------- |\\n| GLUE (SuperGLUE)     | Accuracy |\\n| SQuAD                | EM, F1    |\\n| MNLI                  | Accuracy |\\n| RACE                 | Accuracy |\\n| LAMA                 | Accuracy |\\n| BERTScore            | Pearson |\\n| ROUGE-L              | F1 score |\\n| T5                    | BLEU,\\n   ROUGE-L             |\\n| DPRC                 | Precision, Recall, F1 |\\n| MRC                  | F1 score |\\n| Sentiment Analysis    | Accuracy |\\n| Question Answering   | Accuracy |\\n| Text Classification  | Accuracy |\\n\\nThe document provides the training hyperparameters for a model named Qwen-VL. In the first pre-training stage, the model is trained with a resolution of 224x224, ViT sequence length of 256, and LLM sequence length of 512. The optimizer used is AdamW with a peak learning rate of 2e-4, minimum learning rate of 1e-6, and a cosine learning rate decay. The model uses a batch size of 30720 and lasts for 50,000 steps. In the second pre-training stage, the model's input resolution is increased to 448x448, and the large language model is unlocked. The optimizer, learning rate schedule, and batch size remain the same. The model parallelism techniques are used for ViT and LLM. The evaluation benchmarks used include GLUE (SuperGLUE), SQuAD, MNLI, RACE, LAMA, BERTScore, ROUGE-L, T5, DPRC, MRC, Sentiment Analysis, Question Answering, and Text Classification, with corresponding metrics such as accuracy, F1 score, and Pearson correlation.\"), 0.016666666666666666), (Document(metadata={'doc_id': '3632648b-9899-47df-ae06-9ce65221a9a5'}, page_content='<im_start>user\\nPicture id: 0 Picture 1: <img>vg/VG_100K_2/649.jpg</img>What is the sign in the picture?<im_end>\\n<im_start>assistant\\nThe sign is a road closure with an orange rhombus.<im_end>\\n<im_start>user\\nPicture id: 1 Picture 2: <img>vg/VG_100K_2/823.jpg</img>What is the color of the car in the picture?<im_end>\\n<im_start>assistant\\nThe car is red.<im_end>\\n<im_start>user\\nPicture id: 2 Picture 3: <img>vg/VG_100K_2/731.jpg</img>What is the animal in the picture?<im_end>\\n<im_start>assistant\\nThe animal is a giraffe.<im_end>\\n<im_start>user\\nPicture id: 3 Picture 4: <img>vg/VG_100K_2/428.jpg</img>What is the fruit in the picture?<im_end>\\n<im_start>assistant\\nThe fruit is an apple.<im_end>\\n<im_start>user\\nPicture id: 4 Picture 5: <img>vg/VG_100K_2/694.jpg</img>What is the sign in the picture?<im_end>\\n<im_start>assistant\\nThe sign is a warning sign with a triangle and an exclamation mark.<im_end>\\n<im_start>user\\nPicture id: 5 Picture 6: <img>vg/VG_100K_2/177.jpg</img>What is the color of the house in the picture?<im_end>\\n<im_start>assistant\\nThe house is brown.<im_end>\\n<im_start>user\\nPicture id: 6 Picture 7: <img>vg/VG_100K_2/264.jpg</img>What is the animal in'), 0.01639344262295082), (Document(metadata={'doc_id': 'e2ec72fc-1174-4c63-aae2-589e0f8f039b'}, page_content='\\n0\\n1\\n2\\n4\\n8\\n0\\n1\\n2\\n8\\n0\\n1\\n2\\n4\\n8\\n0\\n1\\n2\\n8\\n0\\n1\\n2\\n4\\n\\nPlease note that the document provided is a summary of the research findings from a paper. The actual paper may contain more detailed information and context. The document describes the results of various models on Text-oriented Question Answering (TextVQA, DocVQA, ChartQA, AI2D, OCR-VQA), Referring Expression Comprehension (RefCOCO, RefCOCO+, RefCOCOg, GRIT, val, test-A test-B), and few-shot learning (OKVQA, Vizwiz, TextVQA, Flickr30k) tasks. The models are categorized as generalist or specialist models based on their architecture and training methodology. The document also compares the performance of these models against each other and against previous state-of-the-art models. The research findings suggest that the models developed in the study obtain top-tier results on all benchmarks, and exhibit satisfactory few-shot learning ability.'), 0.01639344262295082), (Document(metadata={'doc_id': '825d5218-8e94-453c-acae-3c66990cedda'}, page_content=\"| Table 9: Evaluation benchmarks and corresponding metrics |\\n| Evaluation Benchmark | Metrics |\\n| ---------------------| -------- |\\n| GLUE (SuperGLUE)     | Accuracy |\\n| SQuAD                | EM, F1    |\\n| MNLI                  | Accuracy |\\n| RACE                 | Accuracy |\\n| LAMA                 | Accuracy |\\n| BERTScore            | Pearson |\\n| ROUGE-L              | F1 score |\\n| T5                    | BLEU,\\n   ROUGE-L             |\\n| DPRC                 | Precision, Recall, F1 |\\n| MRC                  | F1 score |\\n| Sentiment Analysis    | Accuracy |\\n| Question Answering   | Accuracy |\\n| Text Classification  | Accuracy |\\n\\nThe document provides the training hyperparameters for a model named Qwen-VL. In the first pre-training stage, the model is trained with a resolution of 224x224, ViT sequence length of 256, and LLM sequence length of 512. The optimizer used is AdamW with a peak learning rate of 2e-4, minimum learning rate of 1e-6, and a cosine learning rate decay. The model uses a batch size of 30720 and lasts for 50,000 steps. In the second pre-training stage, the model's input resolution is increased to 448x448, and the large language model is unlocked. The optimizer, learning rate schedule, and batch size remain the same. The model parallelism techniques are used for ViT and LLM. The evaluation benchmarks used include GLUE (SuperGLUE), SQuAD, MNLI, RACE, LAMA, BERTScore, ROUGE-L, T5, DPRC, MRC, Sentiment Analysis, Question Answering, and Text Classification, with corresponding metrics such as accuracy, F1 score, and Pearson correlation.\"), 0.01639344262295082), (Document(metadata={'doc_id': '2047b06e-c983-408f-87ca-9816ebcd71c2'}, page_content='\\n13\\n14\\nIn this work, we introduce the Qwen-VL series, a set of large-scale\\nvision-language models (LVLMs) designed to perceive and\\nunderstand both texts and images. Starting from the Qwen-LM\\nCan you describe the person in the picture?\\nas a foundation, we assign it visual capacity by meticulously designed\\n(i) visual receptor, (ii) input-output interface, (iii) 3-stage training\\nThe person in the picture is wearing a black coat and looks quite\\npipeline, and (iv) multilingual multimodal cleaned corpus. Beyond\\nrelaxed.\\nthe conventional description and question-answering, we inject the\\ngrounding ability into Qwen-VLs by importing fine-grained image-caption-box\\npairs. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new\\nrecords on a broad range of visual-centric benchmarks (e.g., image captioning,\\nquestion answering, visual grounding) under different settings (e.g., zero-shot,\\nfew-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned\\nQwen-VL-Chat also demonstrates conspicuous superiority compared to\\nexisting vision-language chatbots. All models will be made public to facilitate\\nfuture research.\\n\\nThe text introduces the Qwen-VL and Owen-VL series, which are sets of large-scale vision-language models designed to perceive and understand both texts and images. The models are built on foundational LVMs and are enhanced with visual capacity through visual receptors, input-output interfaces, 3-stage training pipelines, and multilingual multimodal cleaned corpora. The grounding ability is added to the models through fine-grained image-caption-box pairs. The models have set new records on various visual-centric benchmarks and demonstrate superiority on real-world dialog benchmarks compared to existing vision-language chatbots. All models will be made public for future research. The text also includes examples of qualitative outputs from the models.'), 0.016129032258064516), (Document(metadata={'doc_id': '54f127b1-dcfc-4c12-9714-c7357cd7c920'}, page_content=\"Text Input: The text input is processed by the large language model, which generates responses based on the\\nimage features, bounding box inputs, and the text itself.\\nOutput: The output is a textual response, which can be a question answer, a region description, or a caption.\\n\\nThis document describes the Qwen-VL model, which is designed for fine-grained image comprehension. The model uses a vision encoder and adapter to extract image features, which are then compressed and fed into a large language model. To maintain positional information during compression, 2D absolute positional encodings are incorporated into the cross-attention mechanism's query-key pairs. The model also handles bounding box inputs and outputs, which require accurate understanding and generation of region descriptions. The text input is processed by the large language model, and the output is a textual response. The document provides details of the Qwen-VL model's parameters, including the vision encoder, VL adapter, and large language model, as well as the training pipeline.\"), 0.016129032258064516), (Document(metadata={'doc_id': '58009ca6-baba-4e54-9f59-bb7480b2171b'}, page_content='\\n13\\n14\\nIn this work, we introduce the Qwen-VL series, a set of large-scale\\nvision-language models (LVLMs) designed to perceive and\\nunderstand both texts and images. Starting from the Qwen-LM\\nCan you describe the person in the picture?\\nas a foundation, we assign it visual capacity by meticulously designed\\n(i) visual receptor, (ii) input-output interface, (iii) 3-stage training\\nThe person in the picture is wearing a black coat and looks quite\\npipeline, and (iv) multilingual multimodal cleaned corpus. Beyond\\nrelaxed.\\nthe conventional description and question-answering, we inject the\\ngrounding ability into Qwen-VLs by importing fine-grained image-caption-box\\npairs. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new\\nrecords on a broad range of visual-centric benchmarks (e.g., image captioning,\\nquestion answering, visual grounding) under different settings (e.g., zero-shot,\\nfew-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned\\nQwen-VL-Chat also demonstrates conspicuous superiority compared to\\nexisting vision-language chatbots. All models will be made public to facilitate\\nfuture research.\\n\\nThe text introduces the Qwen-VL and Owen-VL series, which are sets of large-scale vision-language models designed to perceive and understand both texts and images. The models are built on foundational LVMs and are enhanced with visual capacity through visual receptors, input-output interfaces, 3-stage training pipelines, and multilingual multimodal cleaned corpora. The grounding ability is added to the models through fine-grained image-caption-box pairs. The models have set new records on various visual-centric benchmarks and demonstrate superiority on real-world dialog benchmarks compared to existing vision-language chatbots. All models will be made public for future research. The text also includes examples of qualitative outputs from the models.'), 0.015873015873015872), (Document(metadata={'doc_id': '0089599e-adde-4094-9630-bbe876dcaf0e'}, page_content=\"Text Input: The text input is processed by the large language model, which generates responses based on the\\nimage features, bounding box inputs, and the text itself.\\nOutput: The output is a textual response, which can be a question answer, a region description, or a caption.\\n\\nThis document describes the Qwen-VL model, which is designed for fine-grained image comprehension. The model uses a vision encoder and adapter to extract image features, which are then compressed and fed into a large language model. To maintain positional information during compression, 2D absolute positional encodings are incorporated into the cross-attention mechanism's query-key pairs. The model also handles bounding box inputs and outputs, which require accurate understanding and generation of region descriptions. The text input is processed by the large language model, and the output is a textual response. The document provides details of the Qwen-VL model's parameters, including the vision encoder, VL adapter, and large language model, as well as the training pipeline.\"), 0.015873015873015872)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "rC26olyH1__B",
        "outputId": "ff15fd9d-73c1-4c51-8b70-86b664e669c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nAnswer: Qwen is a large-scale vision-language model (LVLM) designed to perceive and understand both texts and images. It is built on a foundational LVM and is enhanced with visual capacity through visual receptors, input-output interfaces, 3-stage training pipelines, and multilingual multimodal cleaned corpora. The grounding ability is added to the model through fine-grained image-caption-box pairs. The model uses a vision encoder and adapter to extract image features, which are then compressed and fed into a large language model. 2D absolute positional encodings are incorporated into the cross-attention mechanism's query-key pairs to maintain positional information during compression. The model handles bounding box inputs and outputs, which require accurate understanding and generation of region descriptions. The text input is processed by the large language model, and the output is a textual response. The model uses AdamW optimizer with a peak learning rate of 2e-4, minimum learning rate of 1e-6, and a cosine learning rate decay. The batch size is 30720, and the model undergoes 50,000 steps in the first pre-training stage with a resolution of 224x224 and ViT sequence length of 256, and LLM sequence length of 512. In the second pre-training stage, the input resolution is increased to 448x448, and the large language model is unlocked. Model parallelism techniques are used for ViT and LLM. The evaluation benchmarks used include GLUE (SuperGLUE), SQuAD, MNLI, RACE, LAMA, BERTScore, ROUGE-L, T5, DPRC, MRC, Sentiment Analysis, Question Answering, and Text Classification, with corresponding metrics such as accuracy, F1 score, and Pearson correlation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    }
  ]
}